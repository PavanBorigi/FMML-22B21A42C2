{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PavanBorigi/FMML-22B21A42C2/blob/main/FMML_Module_9_Lab_3.ipynb%20module%209-3\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Module 9: Convolutional Neural Networks\n",
        "## **Lab 3**\n",
        "### Module coordinator: Kushagra Agarwal"
      ],
      "metadata": {
        "id": "kCpbL40ggQf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Understanding Convolutions"
      ],
      "metadata": {
        "id": "0hAW8ptqVeyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://miro.medium.com/max/464/0*e-SMFTzO8r7skkpc\" width=650px/>"
      ],
      "metadata": {
        "id": "q6wfvhccKxWx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "yZD5S7IQgHbU"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing some pytorch packages\n",
        "import torch\n",
        "from torch.nn import Conv2d"
      ],
      "metadata": {
        "id": "BDE4WBHalreb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Central to CNNs, a convolution operation is a linear element-wise multiplication operation between a small filter/kernel and same-sized patch from the image. We move this filter over the image like a sliding window from top left to bottom right. For each point on the image, a value is calculated based on the filter using a convolution operation. These filters can do simplest task like checking if there is a vertical line in the image or complicated task like detecting a human eye in the image.\n",
        "\n",
        "Let's look at the convolution formula:\n",
        "\n",
        "Convolution between image\n",
        "$f(x, y)$ and kernel $k(x, y)$ is\n",
        "$$f(x,y) * k(x,y) = \\sum \\limits _{i=0} ^{W-1} \\sum \\limits _{j=0} ^{H-1} f(i, j) k(x − i, y − j)$$\n",
        "\n",
        "where $W$ and $H$ are the the width and height of the image.\n",
        "\n",
        "The code demonstrates the convolution operation of a 2D matrix (image) with various filters"
      ],
      "metadata": {
        "id": "hbpRXyTpVv7u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.researchgate.net/profile/Chaim-Baskin/publication/318849314/figure/fig1/AS:614287726870532@1523469015098/Image-convolution-with-an-input-image-of-size-7-7-and-a-filter-kernel-of-size-3-3.png\" alt=\"Convolution\" width=650px height=280px/>"
      ],
      "metadata": {
        "id": "amI6DTS0Ksvo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D 3x3 binary image with vertical edge\n",
        "image1 = np.array([[1,1,0], [1,1,0], [1,1,0]])\n",
        "\n",
        "# 2D 3x3 binary image with horizontal edge\n",
        "image2 = np.array([[0,0,0], [0,0,0], [1,1,1]])\n",
        "\n",
        "# On plotting the images\n",
        "plt.imshow(image1, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()\n",
        "plt.imshow(image2, cmap='gray', extent=[0, 3, 3, 0])\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "IalqupPPkDil",
        "outputId": "4f971377-7710-4a71-a710-7b3168ef81bd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Vertical Line filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "# Applying filter to first image\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "# Applying filter to second image\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g42INjCaketK",
        "outputId": "e614c373-81b4-40d5-f0f6-3e07d2ece966"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first image:  3\n",
            "Output from second image:  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Horizontal edge filter\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tba3ySYUk2df",
        "outputId": "dd7a6cce-4d43-4647-9bc9-16294ff62d6d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output from first image:  0\n",
            "Output from second image:  3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Non-zero output suggests that there is a vertical edge present in the first image and not present in the second image. Similarly, horizontal edge is detected in second."
      ],
      "metadata": {
        "id": "BmYcPhDgk_in"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's define a function to use convolution layer from Pytorch and use our own kernel to detect edges in image"
      ],
      "metadata": {
        "id": "UNdrDtAKqyj2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_conv(image, kernel, padding=0, stride=1):\n",
        "\n",
        "  #--------IMAGE PREPROCESSING-------\n",
        "  image = torch.from_numpy(image)\n",
        "  # Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  input = image.view((1,1,image.shape[0], image.shape[1]))\n",
        "\n",
        "  # --------------KERNEL-------------\n",
        "  kernel = torch.from_numpy(kernel.astype(np.float32))\n",
        "\n",
        "  # Pytorch requires kernel of shape (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  kernel = kernel.view((1,1,kernel.shape[0], kernel.shape[1]))\n",
        "\n",
        "  # ---------CONVOLUTION LAYER from Pytorch--------\n",
        "  conv = Conv2d(in_channels=1, out_channels=1, kernel_size=kernel.shape, padding=padding, stride=stride)\n",
        "\n",
        "  # Set the kernel weights in the convolution layer\n",
        "  conv.weight = torch.nn.Parameter(kernel)\n",
        "\n",
        "  # ---------APPLY CONVOLUTION--------\n",
        "  output = conv(input.float())\n",
        "  output_img = output.data.numpy()  # Tensor to back in numpy\n",
        "  output_img = output_img.reshape((-1, output_img.shape[-1])) # Reshape to 2D image\n",
        "\n",
        "  return output_img"
      ],
      "metadata": {
        "id": "G5fRJziBk3YB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Our original lotus image\n",
        "image = cv2.imread('/content/grid1 (1).jpg', 0)\n",
        "\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "out1 = apply_conv(image, filter, padding=0, stride=1)\n",
        "\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "out2 = apply_conv(image, filter, padding=0, stride=1)"
      ],
      "metadata": {
        "id": "1HPV6fFZloyc",
        "outputId": "b29cb51f-41fa-491d-b4bc-051f2ef7021d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "expected np.ndarray (got NoneType)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-53b9a77372c3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                    [ 1, 1, 1]])\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mout1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m filter = np.array([[1,0,-1],\n",
            "\u001b[0;32m<ipython-input-14-c272bd410f3f>\u001b[0m in \u001b[0;36mapply_conv\u001b[0;34m(image, kernel, padding, stride)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0;31m#--------IMAGE PREPROCESSING-------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0;31m# Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected np.ndarray (got NoneType)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,6))\n",
        "ax = fig.add_subplot(1,3,1)\n",
        "ax.imshow(image, cmap='gray')\n",
        "ax.set_title('Original Image')\n",
        "ax = fig.add_subplot(1,3,2)\n",
        "ax.set_title('Horizontal edge')\n",
        "ax.imshow(out1, cmap='gray')\n",
        "ax = fig.add_subplot(1,3,3)\n",
        "ax.imshow(out2, cmap='gray')\n",
        "ax.set_title('Vertical edge')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xgwXwbUKnmEr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 633
        },
        "outputId": "c94bc939-ec3b-42ea-afe0-ef55558a894f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Image data of dtype object cannot be converted to float",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-d25eb595e751>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Original Image'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1440\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, interpolation_stage, filternorm, filterrad, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5663\u001b[0m                               **kwargs)\n\u001b[1;32m   5664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5665\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5666\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5667\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    699\u001b[0m         if (self._A.dtype != np.uint8 and\n\u001b[1;32m    700\u001b[0m                 not np.can_cast(self._A.dtype, float, \"same_kind\")):\n\u001b[0;32m--> 701\u001b[0;31m             raise TypeError(\"Image data of dtype {} cannot be converted to \"\n\u001b[0m\u001b[1;32m    702\u001b[0m                             \"float\".format(self._A.dtype))\n\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Image data of dtype object cannot be converted to float"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pooling Layers\n",
        "\n",
        "Pooling layers are used to reduce the dimensions of the feature maps. Thus, it reduces the number of parameters to learn and the amount of computation performed in the network. The pooling layer summarises the features present in a region of the feature map generated by a convolution layer.\n",
        "\n",
        "1) Max Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png' height=150px/>\n",
        "\n",
        "2) Average Pooling:\n",
        "\n",
        "<img src='https://media.geeksforgeeks.org/wp-content/uploads/20190721030705/Screenshot-2019-07-21-at-3.05.56-AM.png' height=150px/>"
      ],
      "metadata": {
        "id": "FpA0yEk1BgRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Softmax layer/activation\n",
        "Recall that logistic regression produces a decimal between 0 and 1.0. For example, a logistic regression output of 0.8 from an email classifier suggests an 80% chance of an email being spam and a 20% chance of it being not spam. Clearly, the sum of the probabilities of an email being either spam or not spam is 1.0.\n",
        "\n",
        "Softmax extends this idea into a multi-class world. That is, Softmax assigns decimal probabilities to each class in a multi-class problem. Those decimal probabilities must add up to 1.0. This additional constraint helps training converge more quickly than it otherwise would.\n",
        "Softmax is implemented through a neural network layer just before the output layer. The Softmax layer must have the same number of nodes as the output layer.\n",
        "\n",
        "<img src='https://miro.medium.com/max/1400/1*ReYpdIZ3ZSAPb2W8cJpkBg.jpeg' height=170px />"
      ],
      "metadata": {
        "id": "eu3QIU7AEO_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning to train a CNN network"
      ],
      "metadata": {
        "id": "P6grxC0TKKSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "qlO-uZUHnn_-"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  Images returned from torchvision dataset classes is in range [0,1]\n",
        "# We transform them to tensors and normalize them to range [-1,1] using 'Normalize' transform\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "# Classes in CIFAR10\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "metadata": {
        "id": "NnezCUbwGqzd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae01aeb-0c49-4375-b2bb-7c504bdc85e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:02<00:00, 58270860.23it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('Training data shape : ', trainset.data.shape, len(trainset.targets))\n",
        "print('Testing data shape : ', testset.data.shape, len(testset.targets))\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ],
      "metadata": {
        "id": "e2M57DhHGupn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3878435-8b4a-47d9-93dc-24d4b5a643d2"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape :  (50000, 32, 32, 3) 50000\n",
            "Testing data shape :  (10000, 32, 32, 3) 10000\n",
            "Total number of outputs :  10\n",
            "Output classes :  ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      # Average loss and acc values\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ],
      "metadata": {
        "id": "_haw697lHCZs"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_model(model, testloader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "  with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        # Calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        # The class with the highest value is what we choose as prediction\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "  print('Accuracy of the network: %d %%' % (\n",
        "      100 * correct / total))"
      ],
      "metadata": {
        "id": "x1Wi6vW7IHcR"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CNN with 2 CONV layers and 3 FC layers\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "        self.fc1 = nn.Linear(32 * 5 * 5, 512)\n",
        "        self.fc2 = nn.Linear(512, 128)\n",
        "        # output layer 10 classes\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        # flatten all dimensions except batch\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RgxbRadcHIms"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "print(model)"
      ],
      "metadata": {
        "id": "02meBxVOHLNL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30ffebea-9624-41af-d1a4-d8b02daee1aa"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=800, out_features=512, bias=True)\n",
            "  (fc2): Linear(in_features=512, out_features=128, bias=True)\n",
            "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "lfKHypeYHNHO"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)"
      ],
      "metadata": {
        "id": "MuDnJL28HPKP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5  # iterations\n",
        "train_losses, train_acc = train(num_epochs, model, trainloader, criterion, optimizer)"
      ],
      "metadata": {
        "id": "AgKhwMrtHRCn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "7e6e78a6-5a50-4e06-e827-e47118eeeb1d"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 0.0131, Accuracy: 38.0920\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-032e78fd03f4>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m  \u001b[0;31m# iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-20-f2ba3bda9914>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(num_epochs, model, train_loader, loss_func, optimizer)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0;31m# Backpropagation, compute gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m       \u001b[0;31m# Apply gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "ax = fig.add_subplot(1,2, 2)\n",
        "ax.plot(np.arange(1,len(train_acc)+1),train_acc)\n",
        "plt.xlabel('Training accuracy')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Accuracy vs Epochs')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tM2wHKGuHToB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        },
        "outputId": "d99b335b-2695-48e4-b0ff-ebeca4349070"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_losses' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-96219723a2ed>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epochs'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAFlCAYAAADbKY7VAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAZtUlEQVR4nO3df2zV1f3H8Vdb6C1GWnBdb0t3tQPnT5RiK11BYlzubKKp44/FTgztGn9M7YxyswkVaEWUMqekiRSJqNM/dMUZMUaaquskRu1CLDTRCRgs2s54C53jXla0hd7z/cN4/VZa5FN636Xl+UjuHz2ez/2ce1LvM5/Lvb1JzjknAACMJI/1AgAAZxbCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMOU5PG+//bZKS0s1Y8YMJSUl6ZVXXvnBY7Zv364rrrhCPp9P559/vp599tkRLBUAMBF4Dk9vb6/mzJmjhoaGk5q/f/9+XX/99brmmmvU3t6ue++9V7feeqtef/11z4sFAIx/SafyR0KTkpK0detWLVq0aNg5y5Yt07Zt2/Thhx/Gx37zm9/o0KFDam5uHumpAQDj1KREn6C1tVXBYHDQWElJie69995hj+nr61NfX1/851gspi+//FI/+tGPlJSUlKilAgC+xzmnw4cPa8aMGUpOHp23BSQ8POFwWH6/f9CY3+9XNBrVV199pSlTphx3TF1dnVavXp3opQEATlJXV5d+8pOfjMp9JTw8I1FdXa1QKBT/ORKJ6Nxzz1VXV5fS09PHcGUAcGaJRqMKBAKaOnXqqN1nwsOTnZ2t7u7uQWPd3d1KT08f8mpHknw+n3w+33Hj6enphAcAxsBo/jNHwj/HU1xcrJaWlkFjb775poqLixN9agDAachzeP73v/+pvb1d7e3tkr55u3R7e7s6OzslffMyWXl5eXz+HXfcoY6ODt13333as2ePNm7cqBdffFFLly4dnUcAABhXPIfn/fff19y5czV37lxJUigU0ty5c1VTUyNJ+uKLL+IRkqSf/vSn2rZtm958803NmTNHjz32mJ566imVlJSM0kMAAIwnp/Q5HivRaFQZGRmKRCL8Gw8AGErE8y9/qw0AYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgKkRhaehoUF5eXlKS0tTUVGRduzYccL59fX1uvDCCzVlyhQFAgEtXbpUX3/99YgWDAAY3zyHZ8uWLQqFQqqtrdXOnTs1Z84clZSU6MCBA0POf+GFF7R8+XLV1tZq9+7devrpp7Vlyxbdf//9p7x4AMD44zk869ev12233abKykpdcskl2rRpk8466yw988wzQ85/7733tGDBAi1evFh5eXm69tprddNNN/3gVRIAYGLyFJ7+/n61tbUpGAx+dwfJyQoGg2ptbR3ymPnz56utrS0emo6ODjU1Nem6664b9jx9fX2KRqODbgCAiWGSl8k9PT0aGBiQ3+8fNO73+7Vnz54hj1m8eLF6enp01VVXyTmnY8eO6Y477jjhS211dXVavXq1l6UBAMaJhL+rbfv27Vq7dq02btyonTt36uWXX9a2bdu0Zs2aYY+prq5WJBKJ37q6uhK9TACAEU9XPJmZmUpJSVF3d/eg8e7ubmVnZw95zKpVq7RkyRLdeuutkqTLLrtMvb29uv3227VixQolJx/fPp/PJ5/P52VpAIBxwtMVT2pqqgoKCtTS0hIfi8ViamlpUXFx8ZDHHDly5Li4pKSkSJKcc17XCwAY5zxd8UhSKBRSRUWFCgsLNW/ePNXX16u3t1eVlZWSpPLycuXm5qqurk6SVFpaqvXr12vu3LkqKirSvn37tGrVKpWWlsYDBAA4c3gOT1lZmQ4ePKiamhqFw2Hl5+erubk5/oaDzs7OQVc4K1euVFJSklauXKnPP/9cP/7xj1VaWqqHH3549B4FAGDcSHLj4PWuaDSqjIwMRSIRpaenj/VyAOCMkYjnX/5WGwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgaUXgaGhqUl5entLQ0FRUVaceOHSecf+jQIVVVVSknJ0c+n08XXHCBmpqaRrRgAMD4NsnrAVu2bFEoFNKmTZtUVFSk+vp6lZSUaO/evcrKyjpufn9/v375y18qKytLL730knJzc/XZZ59p2rRpo7F+AMA4k+Scc14OKCoq0pVXXqkNGzZIkmKxmAKBgO6++24tX778uPmbNm3Sn//8Z+3Zs0eTJ08e0SKj0agyMjIUiUSUnp4+ovsAAHiXiOdfTy+19ff3q62tTcFg8Ls7SE5WMBhUa2vrkMe8+uqrKi4uVlVVlfx+v2bPnq21a9dqYGBg2PP09fUpGo0OugEAJgZP4enp6dHAwID8fv+gcb/fr3A4POQxHR0deumllzQwMKCmpiatWrVKjz32mB566KFhz1NXV6eMjIz4LRAIeFkmAOA0lvB3tcViMWVlZenJJ59UQUGBysrKtGLFCm3atGnYY6qrqxWJROK3rq6uRC8TAGDE05sLMjMzlZKSou7u7kHj3d3dys7OHvKYnJwcTZ48WSkpKfGxiy++WOFwWP39/UpNTT3uGJ/PJ5/P52VpAIBxwtMVT2pqqgoKCtTS0hIfi8ViamlpUXFx8ZDHLFiwQPv27VMsFouPffzxx8rJyRkyOgCAic3zS22hUEibN2/Wc889p927d+vOO+9Ub2+vKisrJUnl5eWqrq6Oz7/zzjv15Zdf6p577tHHH3+sbdu2ae3ataqqqhq9RwEAGDc8f46nrKxMBw8eVE1NjcLhsPLz89Xc3Bx/w0FnZ6eSk7/rWSAQ0Ouvv66lS5fq8ssvV25uru655x4tW7Zs9B4FAGDc8Pw5nrHA53gAYGyM+ed4AAA4VYQHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATI0oPA0NDcrLy1NaWpqKioq0Y8eOkzqusbFRSUlJWrRo0UhOCwCYADyHZ8uWLQqFQqqtrdXOnTs1Z84clZSU6MCBAyc87tNPP9Uf/vAHLVy4cMSLBQCMf57Ds379et12222qrKzUJZdcok2bNumss87SM888M+wxAwMDuvnmm7V69WrNnDnzlBYMABjfPIWnv79fbW1tCgaD391BcrKCwaBaW1uHPe7BBx9UVlaWbrnllpM6T19fn6LR6KAbAGBi8BSenp4eDQwMyO/3Dxr3+/0Kh8NDHvPOO+/o6aef1ubNm0/6PHV1dcrIyIjfAoGAl2UCAE5jCX1X2+HDh7VkyRJt3rxZmZmZJ31cdXW1IpFI/NbV1ZXAVQIALE3yMjkzM1MpKSnq7u4eNN7d3a3s7Ozj5n/yySf69NNPVVpaGh+LxWLfnHjSJO3du1ezZs067jifzyefz+dlaQCAccLTFU9qaqoKCgrU0tISH4vFYmppaVFxcfFx8y+66CJ98MEHam9vj99uuOEGXXPNNWpvb+clNAA4A3m64pGkUCikiooKFRYWat68eaqvr1dvb68qKyslSeXl5crNzVVdXZ3S0tI0e/bsQcdPmzZNko4bBwCcGTyHp6ysTAcPHlRNTY3C4bDy8/PV3Nwcf8NBZ2enkpP5gwgAgKElOefcWC/ih0SjUWVkZCgSiSg9PX2slwMAZ4xEPP9yaQIAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmRhSehoYG5eXlKS0tTUVFRdqxY8ewczdv3qyFCxdq+vTpmj59uoLB4AnnAwAmNs/h2bJli0KhkGpra7Vz507NmTNHJSUlOnDgwJDzt2/frptuuklvvfWWWltbFQgEdO211+rzzz8/5cUDAMafJOec83JAUVGRrrzySm3YsEGSFIvFFAgEdPfdd2v58uU/ePzAwICmT5+uDRs2qLy8/KTOGY1GlZGRoUgkovT0dC/LBQCcgkQ8/3q64unv71dbW5uCweB3d5CcrGAwqNbW1pO6jyNHjujo0aM655xzvK0UADAhTPIyuaenRwMDA/L7/YPG/X6/9uzZc1L3sWzZMs2YMWNQvL6vr69PfX198Z+j0aiXZQIATmOm72pbt26dGhsbtXXrVqWlpQ07r66uThkZGfFbIBAwXCUAIJE8hSczM1MpKSnq7u4eNN7d3a3s7OwTHvvoo49q3bp1euONN3T55ZefcG51dbUikUj81tXV5WWZAIDTmKfwpKamqqCgQC0tLfGxWCymlpYWFRcXD3vcI488ojVr1qi5uVmFhYU/eB6fz6f09PRBNwDAxODp33gkKRQKqaKiQoWFhZo3b57q6+vV29uryspKSVJ5eblyc3NVV1cnSfrTn/6kmpoavfDCC8rLy1M4HJYknX322Tr77LNH8aEAAMYDz+EpKyvTwYMHVVNTo3A4rPz8fDU3N8ffcNDZ2ank5O8upJ544gn19/fr17/+9aD7qa2t1QMPPHBqqwcAjDueP8czFvgcDwCMjTH/HA8AAKeK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAKcIDADBFeAAApggPAMAU4QEAmCI8AABThAcAYIrwAABMER4AgCnCAwAwRXgAAKYIDwDAFOEBAJgiPAAAU4QHAGCK8AAATBEeAICpEYWnoaFBeXl5SktLU1FRkXbs2HHC+X/729900UUXKS0tTZdddpmamppGtFgAwPjnOTxbtmxRKBRSbW2tdu7cqTlz5qikpEQHDhwYcv57772nm266Sbfccot27dqlRYsWadGiRfrwww9PefEAgPEnyTnnvBxQVFSkK6+8Uhs2bJAkxWIxBQIB3X333Vq+fPlx88vKytTb26vXXnstPvbzn/9c+fn52rRp00mdMxqNKiMjQ5FIROnp6V6WCwA4BYl4/p3kZXJ/f7/a2tpUXV0dH0tOTlYwGFRra+uQx7S2tioUCg0aKykp0SuvvDLsefr6+tTX1xf/ORKJSPpmAwAAdr593vV4jXJCnsLT09OjgYEB+f3+QeN+v1979uwZ8phwODzk/HA4POx56urqtHr16uPGA4GAl+UCAEbJf/7zH2VkZIzKfXkKj5Xq6upBV0mHDh3Seeedp87OzlF74BNBNBpVIBBQV1cXL0F+D3szNPZleOzN0CKRiM4991ydc845o3afnsKTmZmplJQUdXd3Dxrv7u5Wdnb2kMdkZ2d7mi9JPp9PPp/vuPGMjAx+IYaQnp7OvgyDvRka+zI89mZoycmj9+kbT/eUmpqqgoICtbS0xMdisZhaWlpUXFw85DHFxcWD5kvSm2++Oex8AMDE5vmltlAopIqKChUWFmrevHmqr69Xb2+vKisrJUnl5eXKzc1VXV2dJOmee+7R1Vdfrccee0zXX3+9Ghsb9f777+vJJ58c3UcCABgXPIenrKxMBw8eVE1NjcLhsPLz89Xc3Bx/A0FnZ+egS7L58+frhRde0MqVK3X//ffrZz/7mV555RXNnj37pM/p8/lUW1s75MtvZzL2ZXjszdDYl+GxN0NLxL54/hwPAACngr/VBgAwRXgAAKYIDwDAFOEBAJg6bcLDVy0Mzcu+bN68WQsXLtT06dM1ffp0BYPBH9zH8czr78y3GhsblZSUpEWLFiV2gWPE674cOnRIVVVVysnJkc/n0wUXXDAh/3/yui/19fW68MILNWXKFAUCAS1dulRff/210WrtvP322yotLdWMGTOUlJR0wr+j+a3t27friiuukM/n0/nnn69nn33W20ndaaCxsdGlpqa6Z555xv3rX/9yt912m5s2bZrr7u4ecv67777rUlJS3COPPOI++ugjt3LlSjd58mT3wQcfGK88sbzuy+LFi11DQ4PbtWuX2717t/vtb3/rMjIy3L///W/jlSee17351v79+11ubq5buHCh+9WvfmWzWENe96Wvr88VFha66667zr3zzjtu//79bvv27a69vd145YnldV+ef/555/P53PPPP+/279/vXn/9dZeTk+OWLl1qvPLEa2pqcitWrHAvv/yyk+S2bt16wvkdHR3urLPOcqFQyH300Ufu8ccfdykpKa65ufmkz3lahGfevHmuqqoq/vPAwICbMWOGq6urG3L+jTfe6K6//vpBY0VFRe53v/tdQtdpzeu+fN+xY8fc1KlT3XPPPZeoJY6ZkezNsWPH3Pz5891TTz3lKioqJmR4vO7LE0884WbOnOn6+/utljgmvO5LVVWV+8UvfjFoLBQKuQULFiR0nWPtZMJz3333uUsvvXTQWFlZmSspKTnp84z5S23fftVCMBiMj53MVy38//nSN1+1MNz88Wgk+/J9R44c0dGjR0f1j/udDka6Nw8++KCysrJ0yy23WCzT3Ej25dVXX1VxcbGqqqrk9/s1e/ZsrV27VgMDA1bLTriR7Mv8+fPV1tYWfzmuo6NDTU1Nuu6660zWfDobjeffMf/r1FZftTDejGRfvm/ZsmWaMWPGcb8k491I9uadd97R008/rfb2doMVjo2R7EtHR4f+8Y9/6Oabb1ZTU5P27dunu+66S0ePHlVtba3FshNuJPuyePFi9fT06KqrrpJzTseOHdMdd9yh+++/32LJp7Xhnn+j0ai++uorTZky5QfvY8yveJAY69atU2Njo7Zu3aq0tLSxXs6YOnz4sJYsWaLNmzcrMzNzrJdzWonFYsrKytKTTz6pgoIClZWVacWKFSf97cAT1fbt27V27Vpt3LhRO3fu1Msvv6xt27ZpzZo1Y720CWHMr3isvmphvBnJvnzr0Ucf1bp16/T3v/9dl19+eSKXOSa87s0nn3yiTz/9VKWlpfGxWCwmSZo0aZL27t2rWbNmJXbRBkbyO5OTk6PJkycrJSUlPnbxxRcrHA6rv79fqampCV2zhZHsy6pVq7RkyRLdeuutkqTLLrtMvb29uv3227VixYpR/YqA8Wa459/09PSTutqRToMrHr5qYWgj2RdJeuSRR7RmzRo1NzersLDQYqnmvO7NRRddpA8++EDt7e3x2w033KBrrrlG7e3tE+abbUfyO7NgwQLt27cvHmJJ+vjjj5WTkzMhoiONbF+OHDlyXFy+jbM7w/+85ag8/3p/38Poa2xsdD6fzz377LPuo48+crfffrubNm2aC4fDzjnnlixZ4pYvXx6f/+6777pJkya5Rx991O3evdvV1tZO2LdTe9mXdevWudTUVPfSSy+5L774In47fPjwWD2EhPG6N983Ud/V5nVfOjs73dSpU93vf/97t3fvXvfaa6+5rKws99BDD43VQ0gIr/tSW1vrpk6d6v7617+6jo4O98Ybb7hZs2a5G2+8caweQsIcPnzY7dq1y+3atctJcuvXr3e7du1yn332mXPOueXLl7slS5bE53/7duo//vGPbvfu3a6hoWF8vp3aOecef/xxd+6557rU1FQ3b948989//jP+366++mpXUVExaP6LL77oLrjgApeamuouvfRSt23bNuMV2/CyL+edd56TdNyttrbWfuEGvP7O/H8TNTzOed+X9957zxUVFTmfz+dmzpzpHn74YXfs2DHjVSeel305evSoe+CBB9ysWbNcWlqaCwQC7q677nL//e9/7ReeYG+99daQzxvf7kdFRYW7+uqrjzsmPz/fpaamupkzZ7q//OUvns7J1yIAAEyN+b/xAADOLIQHAGCK8AAATBEeAIApwgMAMEV4AACmCA8AwBThAQCYIjwAAFOEBwBgivAAAEwRHgCAqf8DeLW3wSn7sEAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Accuracy on test data after training\n",
        "test_model(model, testloader)"
      ],
      "metadata": {
        "id": "3sHK9hhmI-VY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7983a80f-99e7-4533-edbe-7463d5305b04"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network: 58 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "1) List some reasons why we should prefer CNN over ANN for image classification?\n",
        "\n",
        "2) Try improving the CNN performance further by tuning the hyperparameters(epochs, optimizer, LR etc). Report the improved test accuracy.\n",
        "\n",
        "3) What happens if you reduce the number of convolution layers to only 1?\n",
        "\n",
        "4) Why didn't we use the Softmax activation in the last layer of CNN?\n"
      ],
      "metadata": {
        "id": "RBQeCEB6REnH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1st----->\n",
        "Using Convolutional Neural Networks (CNNs) for image classification has several advantages over using traditional Artificial Neural Networks (ANNs):\n",
        "\n",
        "1. **Spatial Hierarchies:** CNNs are designed to capture spatial hierarchies in images. They leverage convolutional layers to detect local patterns (such as edges, textures) and then aggregate this information across the image through pooling layers. ANNs, on the other hand, treat inputs as flattened vectors, ignoring the spatial structure of the image.\n",
        "\n",
        "2. **Parameter Sharing:** CNNs use parameter sharing in convolutional layers, where a single set of weights is applied across the entire image. This reduces the number of parameters compared to fully connected layers in ANNs, making CNNs more efficient in learning and generalization, especially for large images.\n",
        "\n",
        "3. **Translation Invariance:** CNNs are inherently translation invariant due to the local receptive fields of convolutional layers. This means they can recognize patterns regardless of their position in the image. ANNs, without specific design considerations, lack this property.\n",
        "\n",
        "4. **Feature Learning:** CNNs automatically learn hierarchical features from raw pixel values. Through the convolutional and pooling layers, CNNs extract relevant features at different levels of abstraction, which is crucial for tasks like object recognition in images. ANNs require manual feature engineering or dimensionality reduction techniques to achieve similar results.\n",
        "\n",
        "5. **Data Efficiency:** CNNs require fewer parameters compared to ANNs, which makes them more data-efficient, especially when dealing with high-dimensional data like images. This is particularly important when the available dataset is limited, as CNNs can generalize better with fewer training examples.\n",
        "\n",
        "6. **Regularization:** CNN architectures often include techniques like dropout and batch normalization, which help prevent overfitting and improve generalization performance. While these techniques can also be applied to ANNs, they are particularly effective in the context of CNNs due to their parameter-sharing nature.\n",
        "\n",
        "7. **State-of-the-Art Performance:** CNNs have consistently achieved state-of-the-art performance in various image classification tasks, including benchmark datasets like ImageNet. Their ability to learn complex hierarchical features from raw pixel data makes them well-suited for these tasks.\n",
        "\n",
        "Overall, CNNs are specifically designed for processing and analyzing visual data like images, making them the preferred choice for tasks such as image classification, object detection, and segmentation.\n",
        "\n",
        "2nd---->\n",
        "To improve the performance of a Convolutional Neural Network (CNN) for image classification, we can experiment with tuning several hyperparameters such as the number of epochs, choice of optimizer, learning rate, batch size, and network architecture. Here's a general approach to do this:\n",
        "\n",
        "1. **Hyperparameter Tuning**: We can try different values for hyperparameters like the number of epochs, learning rate, and batch size. This can be done through systematic experimentation, grid search, or random search.\n",
        "\n",
        "2. **Optimizer Selection**: Popular optimizers like Adam, RMSprop, and SGD with momentum have different properties and may perform differently depending on the dataset and problem. Experimenting with different optimizers can help find the one that works best for our CNN.\n",
        "\n",
        "3. **Learning Rate Adjustment**: Learning rate determines the step size during optimization. It's crucial to find an appropriate learning rate.\n",
        "\n",
        "3rd----->\n",
        "Reducing the number of convolutional layers to just one in a Convolutional Neural Network (CNN) can have several implications:\n",
        "\n",
        "1. **Reduced Capacity to Learn Hierarchical Features**: CNNs are designed to learn hierarchical features by stacking multiple convolutional layers. Each layer captures different levels of abstraction, starting from simple patterns (edges, textures) in the early layers to complex features (objects, parts of objects) in the deeper layers. With only one convolutional layer, the network's ability to capture complex features may be limited.\n",
        "\n",
        "2. **Limited Receptive Field**: Deep CNNs use multiple convolutional layers with increasing receptive fields to capture contextual information from the input image. With only one convolutional layer, the receptive field would be limited, potentially affecting the network's ability to understand the global context of the image.\n",
        "\n",
        "3. **Potential Overfitting**: With fewer convolutional layers, the network may have less capacity to generalize well to unseen data. Deep CNNs with multiple convolutional layers tend to generalize better due to their ability to learn hierarchical representations and extract relevant features. A single convolutional layer might lead to overfitting, especially if the dataset is complex or limited.\n",
        "\n",
        "4. **Simpler Model**: On the positive side, reducing the number of convolutional layers simplifies the model, which can be beneficial in scenarios where computational resources are limited or when dealing with small datasets. It can also lead to faster training times compared to deeper architectures.\n",
        "\n",
        "Overall, reducing the number of convolutional layers to just one simplifies the CNN architecture but may sacrifice the network's ability to learn complex features and generalize well to new data. The impact of this change would depend on the specific dataset and problem at hand. It's often a trade-off between model complexity and performance.\n",
        "\n",
        "4th----->\n",
        "In the last layer of a CNN for image classification, we typically use the Softmax activation function because it provides a probability distribution over multiple classes, allowing us to interpret the network's output as the likelihood of each class. This makes it suitable for multi-class classification tasks."
      ],
      "metadata": {
        "id": "aLqw2H-H74pL"
      }
    }
  ]
}